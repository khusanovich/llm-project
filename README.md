# ğŸ§  LLM Local Chat with Mistral + Ollama

This project is a minimal example of using a **local Large Language Model (LLM)** with Python and [Ollama](https://ollama.com/). It uses the open-source **Mistral** model to generate text completely offline â€” no API keys, no cloud, just local power ğŸ’ª

---

## ğŸš€ Features

- ğŸ”’ 100% offline LLM inference with [Ollama](https://ollama.com/)
- ğŸ§ª Python script to send prompts to the local model
- âœ… Virtual environment with clean setup
- ğŸ“ Git-tracked project structure

---

## ğŸ› ï¸ Requirements

- Python 3.11+
- [Ollama](https://ollama.com/) installed and running
- Mistral model pulled locally:
  
  ```bash
  ollama pull mistral

