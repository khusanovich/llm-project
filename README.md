# 🧠 LLM Local Chat with Mistral + Ollama

This project is a minimal example of using a **local Large Language Model (LLM)** with Python and [Ollama](https://ollama.com/). It uses the open-source **Mistral** model to generate text completely offline — no API keys, no cloud, just local power 💪

---

## 🚀 Features

- 🔒 100% offline LLM inference with [Ollama](https://ollama.com/)
- 🧪 Python script to send prompts to the local model
- ✅ Virtual environment with clean setup
- 📁 Git-tracked project structure

---

## 🛠️ Requirements

- Python 3.11+
- [Ollama](https://ollama.com/) installed and running
- Mistral model pulled locally:
  
  ```bash
  ollama pull mistral

